# AI Palette ğŸ¨

è½»é‡ä¼˜é›…çš„ç»Ÿä¸€ AI æ¥å£ï¼Œä¸€ä¸ªè°ƒç”¨æ»¡è¶³æ‰€æœ‰éœ€æ±‚ã€‚
æ”¯æŒå¤šç§ä¸»æµ AI æ¨¡å‹ï¼Œå¦‚åŒè°ƒè‰²æ¿ä¸€æ ·ï¼Œéšå¿ƒæ‰€æ¬²åœ°åˆ‡æ¢ä¸åŒçš„ AI æœåŠ¡ã€‚
éå¸¸é€‚åˆåœ¨ Cursor ç­‰ AI IDE ä½œä¸ºä¸Šä¸‹æ–‡ä½¿ç”¨ã€‚

## ğŸŒŸ ä¸ºä»€ä¹ˆé€‰æ‹© AI Palette?

- ğŸ”„ **ç»Ÿä¸€æ¥å£**: ä¸€å¥—ä»£ç é€‚é…å¤šä¸ªå¤§æ¨¡å‹ï¼Œæ— éœ€é‡å¤å¼€å‘
- ğŸ›  **é™ä½æˆæœ¬**: çµæ´»åˆ‡æ¢ä¸åŒæ¨¡å‹ï¼Œä¼˜åŒ–ä½¿ç”¨æˆæœ¬
- ğŸš€ **å¿«é€Ÿæ¥å…¥**: 5åˆ†é’Ÿå³å¯å®Œæˆæ¥å…¥ï¼Œæ”¯æŒæµå¼è¾“å‡º
- ğŸ”Œ **é«˜å¯ç”¨æ€§**: å†…ç½®å®Œå–„çš„é‡è¯•æœºåˆ¶ï¼Œç¡®ä¿æœåŠ¡ç¨³å®šæ€§
- ğŸ¯ **å¼€ç®±å³ç”¨**: ä¸»æµæ¨¡å‹å¼€ç®±å³ç”¨ï¼Œæ¥å£ç»Ÿä¸€è§„èŒƒ

## âœ¨ ç‰¹æ€§

- ğŸ¨ ç»Ÿä¸€ä¼˜é›…çš„æ¥å£è®¾è®¡
- ğŸ’ å•æ–‡ä»¶å®ç°ï¼Œè½»é‡çº§ä¸”æ–¹ä¾¿é›†æˆ
- ğŸŒŠ æ”¯æŒæµå¼è¾“å‡º
- ğŸ”„ å®Œå–„çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
- ğŸ“ ç±»å‹æç¤ºå’Œæ–‡æ¡£å®Œå¤‡
- âš™ï¸ é…ç½®çµæ´»ï¼Œæ”¯æŒç›´æ¥ä¼ å‚å’Œç¯å¢ƒå˜é‡
- ğŸ’¬ æ”¯æŒä¸Šä¸‹æ–‡å¯¹è¯

## ğŸ¯ æ”¯æŒçš„æ¨¡å‹

### OpenAI
- GPT-4 Turbo
- GPT-3.5 Turbo

### ç™¾åº¦æ–‡å¿ƒä¸€è¨€
- ERNIE Bot 4.0
- ERNIE Bot 8K

### é˜¿é‡Œé€šä¹‰åƒé—®
- Qwen Turbo
- Qwen Plus
- Qwen Max

### æ™ºè°± AI
- GLM-4
- GLM-4-32K

### MiniMax
- ABAB-6
- ABAB-5.5

### DeepSeek
- DeepSeek Chat V3
- DeepSeek Chat R1

### ç¡…åŸºæµåŠ¨ï¼š
- DeepSeek-R1 / V3
- Qwen 2.5 (72B/32B/14B/7B)
- Meta Llama 3 (70B/8B)
- Google Gemma 2 (27B/9B)
- InternLM 2.5 (20B/7B)
- Yi 1.5 (34B/9B/6B)
- ChatGLM 4 (9B)

### Ollama (æœ¬åœ°éƒ¨ç½²)
- Llama 2
- Mistral
- CodeLlama
- Gemma
â€¦â€¦

## ğŸ“¦ å®‰è£…

```bash
pip install -r requirements.txt
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

```python
from ai_palette import AIChat, Message

# æ–¹å¼1ï¼šç›´æ¥ä¼ å…¥é…ç½®
chat = AIChat(
    provider="openai",  # æ”¯æŒ: openai, ernie, dashscope, zhipu, ollama, minimax, deepseek, siliconflow
    model="gpt-3.5-turbo",
    api_key="your-api-key"
)

# æ–¹å¼2ï¼šä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
chat = AIChat(provider="openai")  # ä¼šè‡ªåŠ¨è¯»å–å¯¹åº”çš„ç¯å¢ƒå˜é‡ï¼Œå¦‚ OPENAI_API_KEY å’Œ OPENAI_MODEL

# åŸºæœ¬å¯¹è¯
response = chat.ask("ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±")
print(response)

# å¸¦ç³»ç»Ÿæç¤ºè¯çš„å¯¹è¯
chat.add_context("ä½ æ˜¯ä¸€ä¸ªä¸­åŒ»ä¸“å®¶")
response = chat.ask("å¤´ç—›è¯¥æ€ä¹ˆåŠï¼Ÿ")
print(response)

# æµå¼è¾“å‡º
chat = AIChat(provider="openai", enable_streaming=True)
for chunk in chat.ask("è®²ä¸€ä¸ªæ•…äº‹"):
    print(chunk, end="", flush=True)

# ä¸Šä¸‹æ–‡å¯¹è¯
messages = []
messages.append(Message(role="user", content="ä½ å¥½ï¼Œæˆ‘å«å°æ˜"))
response = chat.ask("ä½ å¥½ï¼Œæˆ‘å«å°æ˜", messages=messages)
messages.append(Message(role="assistant", content=response))

messages.append(Message(role="user", content="ä½ è¿˜è®°å¾—æˆ‘çš„åå­—å—ï¼Ÿ"))
response = chat.ask("ä½ è¿˜è®°å¾—æˆ‘çš„åå­—å—ï¼Ÿ", messages=messages)

# ä¸Šä¸‹æ–‡ç®¡ç†
chat = AIChat(provider="openai")

# æ·»åŠ ç³»ç»Ÿæç¤ºè¯ï¼ˆåªèƒ½æ·»åŠ ä¸€ä¸ªï¼‰
chat.add_context("ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Pythonå¯¼å¸ˆ", role="system")

# æ·»åŠ å¯¹è¯å†å²
chat.add_context("æˆ‘æƒ³å­¦ä¹ Python", role="user")
chat.add_context("å¾ˆå¥½ï¼ŒPythonæ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é€‰æ‹©ã€‚æˆ‘ä»¬ä»åŸºç¡€å¼€å§‹å§ã€‚", role="assistant")

# å‘é€æ–°çš„é—®é¢˜
response = chat.ask("æˆ‘åº”è¯¥ä»å“ªé‡Œå¼€å§‹ï¼Ÿ")

# æ¸…é™¤æ™®é€šä¸Šä¸‹æ–‡ï¼Œä¿ç•™ç³»ç»Ÿæç¤ºè¯
chat.clear_context()

# æ¸…é™¤æ‰€æœ‰ä¸Šä¸‹æ–‡ï¼ˆåŒ…æ‹¬ç³»ç»Ÿæç¤ºè¯ï¼‰
chat.clear_context(include_system_prompt=True)
```

## âš™ï¸ ç¯å¢ƒå˜é‡é…ç½®

åˆ›å»º `.env` æ–‡ä»¶ï¼Œå‚è€ƒ `.env.example` è¿›è¡Œé…ç½®ï¼š

```bash
# OpenAI GPT é…ç½®
GPT_API_KEY=sk-xxxxxxxxxxxxxxxx
GPT_MODEL=gpt-4o-mini

# æ–‡å¿ƒä¸€è¨€é…ç½®
ERNIE_API_KEY=xxxxxxxxxxxxxxxx
ERNIE_API_SECRET=xxxxxxxxxxxxxxxx
ERNIE_MODEL=ernie-bot-4

# é€šä¹‰åƒé—®é…ç½®
# https://bailian.console.aliyun.com/?apiKey=1
DASHSCOPE_API_KEY=xxxxxxxxxxxxxxxx
DASHSCOPE_MODEL=qwen-max

# ChatGLMé…ç½®
# https://open.bigmodel.cn/usercenter/proj-mgmt/apikeys
ZHIPU_API_KEY=xxxxxxxxxxxxxxxx
ZHIPU_MODEL=GLM-4-Plus

# Ollamaé…ç½®
OLLAMA_API_URL=http://localhost:11434/api/chat
OLLAMA_MODEL=first

# MiniMaxé…ç½®
# https://platform.minimaxi.com/user-center/basic-information/interface-key
MINIMAX_API_KEY=xxxxxxxxxxxxxxxx
MINIMAX_API_SECRET=xxxxxxxxxxxxxxxx  # Group ID
MINIMAX_MODEL=abab5.5-chat

# Deepseeké…ç½®
# https://platform.deepseek.com/
DEEPSEEK_API_KEY=xxxxxxxxxxxxxxxx
DEEPSEEK_MODEL=deepseek-reasoner

# Siliconflowé…ç½®
SILICONFLOW_API_KEY=xxxxxxxxxxxxxxxx
SILICONFLOW_MODEL=siliconflow-chat
```

## ğŸ¯ é«˜çº§ç”¨æ³•

### Deepseek æ¨¡å‹ä½¿ç”¨

Deepseek æ¨¡å‹å…·æœ‰ç‹¬ç‰¹çš„æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥å±•ç¤º AI çš„æ€è€ƒè¿‡ç¨‹ï¼š

```python
from ai_palette import AIChat

# åˆ›å»º Deepseek å®ä¾‹
chat = AIChat(
    provider="deepseek",
    model="deepseek-reasoner",
    enable_streaming=True  # å¯ç”¨æµå¼è¾“å‡º
)

# éæµå¼è¯·æ±‚
response = chat.ask("è§£é‡Šé‡å­çº ç¼ ç°è±¡")
print("å›ç­”:", response)
print("æ¨ç†è¿‡ç¨‹:", chat.get_last_reasoning_content())

# æµå¼è¯·æ±‚
for chunk in chat.ask("ä¸ºä»€ä¹ˆæœˆäº®æ€»æ˜¯åŒä¸€é¢æœå‘åœ°çƒï¼Ÿ"):
    if chunk["type"] == "reasoning":
        print("\n[æ¨ç†è¿‡ç¨‹]", chunk["content"], end="")
    else:  # type == "content"
        print("\n[æœ€ç»ˆç­”æ¡ˆ]", chunk["content"], end="")
```

#### Deepseek API Key è®¾ç½®

æœ‰ä¸‰ç§æ–¹å¼è®¾ç½® Deepseek API Keyï¼š

1. å‘½ä»¤è¡Œå‚æ•°ï¼š
```bash
python test_deepseek.py --api-key YOUR_API_KEY --save
```

2. ç¯å¢ƒå˜é‡ï¼š
```bash
export DEEPSEEK_API_KEY="your-api-key"
```

3. äº¤äº’å¼è¾“å…¥ï¼š
ç›´æ¥è¿è¡Œç¨‹åºï¼Œæ ¹æ®æç¤ºè¾“å…¥ API Keyã€‚

#### Deepseek ç‰¹æœ‰åŠŸèƒ½

- æ¨ç†è¿‡ç¨‹å±•ç¤ºï¼šé€šè¿‡ `get_last_reasoning_content()` è·å– AI çš„æ¨ç†è¿‡ç¨‹
- æµå¼è¾“å‡ºåŒºåˆ†ï¼šæ”¯æŒåŒæ—¶è·å–æ¨ç†è¿‡ç¨‹å’Œæœ€ç»ˆç­”æ¡ˆçš„æµå¼è¾“å‡º
- è¶…æ—¶æ§åˆ¶ï¼šå¯ä»¥æ ¹æ®é—®é¢˜å¤æ‚åº¦è®¾ç½®ä¸åŒçš„è¶…æ—¶æ—¶é—´
  ```python
  # å¤æ‚é—®é¢˜ä½¿ç”¨æ›´é•¿çš„è¶…æ—¶æ—¶é—´
  chat = AIChat(
      provider="deepseek",
      model="deepseek-reasoner",
      timeout=180  # 3åˆ†é’Ÿè¶…æ—¶
  )
  ```

### é€‰æ‹©æ€§æµ‹è¯•

å¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡é€‰æ‹©è¦æµ‹è¯•çš„æ¨¡å‹ï¼š

```bash
# åªæµ‹è¯•æŒ‡å®šçš„æ¨¡å‹
export TEST_MODELS=openai,deepseek,ollama
python test_ai_palette.py

# æµ‹è¯•æ‰€æœ‰æ¨¡å‹
python test_ai_palette.py
```

### æ¶ˆæ¯å†å²

```python
messages = [
    Message(role="system", content="ä½ æ˜¯ä¸€ä¸ªhelpfulåŠ©æ‰‹"),
    Message(role="user", content="ä»Šå¤©å¤©æ°”çœŸå¥½"),
    Message(role="assistant", content="æ˜¯çš„ï¼Œé˜³å…‰æ˜åªš")
]
response = chat.ask("æˆ‘ä»¬å»æ•£æ­¥å§", messages=messages)
```

### é”™è¯¯é‡è¯•

é»˜è®¤å¯ç”¨æŒ‡æ•°é€€é¿é‡è¯•æœºåˆ¶ï¼š
- æœ€å¤§é‡è¯•æ¬¡æ•°ï¼š3æ¬¡
- åŸºç¡€å»¶è¿Ÿï¼š1ç§’
- æœ€å¤§å»¶è¿Ÿï¼š10ç§’

å¯ä»¥åœ¨åˆ›å»ºå®ä¾‹æ—¶è‡ªå®šä¹‰ï¼š

```python
chat = AIChat(
    provider="openai",
    retry_count=5,  # æœ€å¤§é‡è¯•5æ¬¡
    timeout=60     # è¯·æ±‚è¶…æ—¶æ—¶é—´60ç§’
)
```

### ä¸Šä¸‹æ–‡ç®¡ç†

AI Palette æä¾›äº†çµæ´»çš„ä¸Šä¸‹æ–‡ç®¡ç†åŠŸèƒ½ï¼š

- **ç³»ç»Ÿæç¤ºè¯**: åªèƒ½è®¾ç½®ä¸€ä¸ªï¼Œå§‹ç»ˆä½äºå¯¹è¯æœ€å‰é¢
- **å¯¹è¯å†å²**: å¯ä»¥æ·»åŠ å¤šæ¡ç”¨æˆ·å’ŒåŠ©æ‰‹çš„å¯¹è¯è®°å½•
- **ä¸Šä¸‹æ–‡æ¸…ç†**: æ”¯æŒé€‰æ‹©æ€§æ¸…é™¤æ™®é€šå¯¹è¯æˆ–åŒ…å«ç³»ç»Ÿæç¤ºè¯

```python
# æ·»åŠ ç³»ç»Ÿæç¤ºè¯
chat.add_context("ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Pythonå¯¼å¸ˆ", role="system")

# æ·»åŠ å¯¹è¯å†å²
chat.add_context("æˆ‘æƒ³å­¦ä¹ Python", role="user")
chat.add_context("å¾ˆå¥½ï¼Œæˆ‘ä»¬å¼€å§‹å§", role="assistant")

# æ¸…é™¤ä¸Šä¸‹æ–‡
chat.clear_context()  # åªæ¸…é™¤æ™®é€šå¯¹è¯
chat.clear_context(include_system_prompt=True)  # æ¸…é™¤æ‰€æœ‰ä¸Šä¸‹æ–‡
```

## ğŸ“„ Web ç•Œé¢

é¡¹ç›®æä¾›äº†ä¸€ä¸ªç®€æ´ç¾è§‚çš„ Web ç•Œé¢ï¼ˆ`app.py`ï¼‰ï¼Œå¯ä»¥ç›´æ¥åœ¨æµè§ˆå™¨ä¸­ä½“éªŒå„ç§æ¨¡å‹çš„å¯¹è¯èƒ½åŠ›ï¼š

```bash
# è¿è¡Œ Web æœåŠ¡
python app.py
```

å¯åŠ¨åè®¿é—® `http://localhost:5000` å³å¯ä½¿ç”¨ã€‚ä¸»è¦åŠŸèƒ½ï¼š
- æ”¯æŒæ‰€æœ‰å·²é…ç½®æ¨¡å‹çš„åœ¨çº¿å¯¹è¯
- æ”¯æŒæµå¼è¾“å‡º
- æ”¯æŒè‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯
- æ”¯æŒæŸ¥çœ‹å¯¹è¯å†å²
- æ”¯æŒå¯¼å‡ºå¯¹è¯è®°å½•


<img src="static/image/web_demo.png" width="600" alt="AI Palette">

## ğŸ“„ è®¸å¯è¯

MIT 


<img src="static/image/connect.jpg" width="400" alt="PI Palette">

---

# AI Palette ğŸ¨ [English]

A lightweight and elegant unified AI interface that meets all needs with a single call.
Supporting multiple mainstream AI models, switch between different AI services as freely as using a palette.
It is great for AI IDEs such as Cursor to use as a context.

## ğŸŒŸ Why Choose AI Palette?

- ğŸ”„ **Unified Interface**: One codebase fits multiple large models, no need to develop repeatedly
- ğŸ›  **Reduce Costs**: Flexible switching between different models, optimizing usage costs
- ğŸš€ **Quick Integration**: 5 minutes to complete integration, support streaming output
- ğŸ”Œ **High Availability**: Built-in complete retry mechanism, ensuring service stability
- ğŸ¯ **Out-of-the-Box**: Mainstream models ready to use, interface uniform and standardized

## âœ¨ Features

- ğŸ¨ Unified elegant interface design
- ğŸ’ Single file implementation, lightweight and easy to integrate
- ğŸŒŠ Support streaming output
- ğŸ”„ Complete error handling and retry mechanism
- ğŸ“ Type hints and comprehensive documentation
- âš™ï¸ Flexible configuration, supporting direct parameters and environment variables
- ğŸ’¬ Support contextual dialogue

## ğŸ¯ Supported Models

### OpenAI
- GPT-4 Turbo
- GPT-3.5 Turbo

### Baidu ERNIE
- ERNIE Bot 4.0
- ERNIE Bot 8K

### Alibaba Qwen
- Qwen Turbo
- Qwen Plus
- Qwen Max

### Zhipu AI
- GLM-4
- GLM-4-32K

### MiniMax
- ABAB-6
- ABAB-5.5

### DeepSeek
- DeepSeek Chat V3
- DeepSeek Chat R1

### SiliconFlow
- DeepSeek-R1 / V3
- Qwen 2.5 (72B/32B/14B/7B)
- Meta Llama 3 (70B/8B)
- Google Gemma 2 (27B/9B)
- InternLM 2.5 (20B/7B)
- Yi 1.5 (34B/9B/6B)
- ChatGLM 4 (9B)

### Ollama (Local Deployment)
- Llama 2
- Mistral
- CodeLlama
- Gemma

## ğŸ“¦ Installation

```bash
pip install -r requirements.txt
```

## ğŸš€ Quick Start

```python
from ai_palette import AIChat, Message

# Method 1: Direct configuration
chat = AIChat(
    provider="openai",  # æ”¯æŒ: openai, ernie, dashscope, zhipu, ollama, minimax, deepseek, siliconflow
    model="gpt-3.5-turbo",
    api_key="your-api-key"
)

# Method 2: Read from environment variables
chat = AIChat(provider="openai")  # Will automatically read OPENAI_API_KEY and OPENAI_MODEL

# Basic conversation
response = chat.ask("Hello, please introduce yourself")
print(response)

# Conversation with system prompt
chat.add_context("You are a medical expert")
response = chat.ask("What should I do for a headache?")
print(response)

# Streaming output
chat = AIChat(provider="openai", enable_streaming=True)
for chunk in chat.ask("Tell me a story"):
    print(chunk, end="", flush=True)

# Contextual dialogue
messages = []
messages.append(Message(role="user", content="Hi, my name is Tom"))
response = chat.ask("Hi, my name is Tom", messages=messages)
messages.append(Message(role="assistant", content=response))

messages.append(Message(role="user", content="Do you remember my name?"))
response = chat.ask("Do you remember my name?", messages=messages)

# Context management
chat = AIChat(provider="openai")

# Add system prompt (only one can be set)
chat.add_context("You are a Python tutor", role="system")

# Add dialogue history
chat.add_context("I want to learn Python", role="user")
chat.add_context("Great, let's start with the basics", role="assistant")

# Send new question
response = chat.ask("Where should I start?")

# Clear regular context, keep system prompt
chat.clear_context()

# Clear all context (including system prompt)
chat.clear_context(include_system_prompt=True)
```

## âš™ï¸ Environment Variables

Create a `.env` file, refer to `.env.example` for configuration:

```bash
# OpenAI GPT Configuration
# https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxx
OPENAI_MODEL=gpt-3.5-turbo

# ERNIE Configuration
# https://cloud.baidu.com/product/wenxinworkshop
ERNIE_API_KEY=xxxxxxxxxxxxxxxx
ERNIE_API_SECRET=xxxxxxxxxxxxxxxx
ERNIE_MODEL=ernie-bot-4

# ChatGLM Configuration
# https://open.bigmodel.cn/usercenter/apikeys
GLM_API_KEY=xxxxxxxxxxxxxxxx
GLM_MODEL=glm-4

# Qwen Configuration
# https://bailian.console.aliyun.com/?apiKey=1
QWEN_API_KEY=xxxxxxxxxxxxxxxx
QWEN_MODEL=qwen-max

# MiniMax Configuration
# https://platform.minimax.com/user-center/basic-information/interface-key
MINIMAX_API_KEY=xxxxxxxxxxxxxxxx
MINIMAX_API_SECRET=xxxxxxxxxxxxxxxx
MINIMAX_MODEL=abab5.5-chat

# Ollama Configuration (No API KEY needed for local running)
# https://ollama.com/download
OLLAMA_API_URL=http://localhost:11434/api/chat
OLLAMA_MODEL=llama2
```

## ğŸ¯ Advanced Usage

### Message History

```python
messages = [
    Message(role="system", content="You are a helpful assistant"),
    Message(role="user", content="The weather is nice today"),
    Message(role="assistant", content="Yes, it's sunny")
]
response = chat.ask("Shall we go for a walk?", messages=messages)
```

### Error Retry

Exponential backoff retry mechanism enabled by default:
- Maximum retry attempts: 3
- Base delay: 1 second
- Maximum delay: 10 seconds

Can be customized when creating an instance:

```python
chat = AIChat(
    provider="openai",
    retry_count=5,  # Maximum 5 retries
    timeout=60     # Request timeout 60 seconds
)
```

### Context Management [English]

AI Palette provides flexible context management features:

- **System Prompt**: Only one can be set, always at the beginning of the conversation
- **Dialogue History**: Multiple user and assistant messages can be added
- **Context Cleanup**: Supports selective clearing of regular dialogue or including system prompt

```python
# Add system prompt
chat.add_context("You are a Python tutor", role="system")

# Add dialogue history
chat.add_context("I want to learn Python", role="user")
chat.add_context("Great, let's start with the basics", role="assistant")

# Clear context
chat.clear_context()  # Only clear regular dialogue
chat.clear_context(include_system_prompt=True)  # Clear all context
```

## ğŸ“„ Web Interface

The project provides a clean and beautiful web interface (`app.py`) that allows you to experience various models' conversational capabilities directly in your browser:

```bash
# Run the web server
python app.py
```

Visit `http://localhost:5000` after startup. Main features:
- Support online conversation with all configured models
- Support streaming output
- Support custom system prompts
- Support viewing conversation history
- Support exporting conversation records

<img src="static/image/web_demo.png" width="600" alt="AI Palette">

## ğŸ“„ License

MIT 

### Deepseek Model Usage

Deepseek model has unique reasoning ability, showing AI's thinking process:

```python
from ai_palette import AIChat

# Create Deepseek instance
chat = AIChat(
    provider="deepseek",
    model="deepseek-reasoner",
    enable_streaming=True  # Enable streaming output
)

# Non-streaming request
response = chat.ask("Explain quantum entanglement phenomenon")
print("Answer:", response)
print("Reasoning:", chat.get_last_reasoning_content())

# Streaming request
for chunk in chat.ask("Why does the moon always face the same side towards the earth?"):
    if chunk["type"] == "reasoning":
        print("\n[Reasoning]", chunk["content"], end="")
    else:  # type == "content"
        print("\n[Final Answer]", chunk["content"], end="")
```

#### Deepseek API Key Setting

There are three ways to set Deepseek API Key:

1. Command line argument:
```bash
python test_deepseek.py --api-key YOUR_API_KEY --save
```

2. Environment variable:
```bash
export DEEPSEEK_API_KEY="your-api-key"
```

3. Interactive input:
Run the program and enter API Key based on the prompt.

#### Deepseek Specific Features

- Reasoning Process Display: Get AI's reasoning process through `get_last_reasoning_content()`
- Streaming Output Differentiation: Support streaming output that simultaneously gets reasoning process and final answer
- Timeout Control: Different timeout times can be set based on the complexity of the question
  ```python
  # Use longer timeout for complex questions
  chat = AIChat(
      provider="deepseek",
      model="deepseek-reasoner",
      timeout=180  # 3 minutes timeout
  )
  ```
```

ä¸€ä¸ªè½»é‡çº§ä¼˜é›…çš„ç»Ÿä¸€ AI æ¥å£ã€‚

## å®‰è£…

```bash
pip install ai-palette
```

## ä½¿ç”¨æ–¹æ³•

å¯åŠ¨æœåŠ¡å™¨æœ‰ä¸¤ç§æ–¹å¼ï¼š

1. ä½¿ç”¨ Python æ¨¡å—æ–¹å¼ï¼ˆæ¨èï¼‰ï¼š
```bash
python -m ai_palette.app
```

2. ä½¿ç”¨å‘½ä»¤è¡Œå·¥å…·ï¼ˆéœ€è¦ç¡®ä¿ Python Scripts ç›®å½•åœ¨ç³»ç»Ÿ PATH ä¸­ï¼‰ï¼š
```bash
ai-palette-server
```

æœåŠ¡å™¨å¯åŠ¨åï¼Œè®¿é—® http://127.0.0.1:18000 å³å¯ä½¿ç”¨ã€‚
