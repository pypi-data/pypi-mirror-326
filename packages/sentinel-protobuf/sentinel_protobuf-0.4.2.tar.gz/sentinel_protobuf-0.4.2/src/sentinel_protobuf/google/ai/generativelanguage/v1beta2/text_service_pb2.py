# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# NO CHECKED-IN PROTOBUF GENCODE
# source: google/ai/generativelanguage/v1beta2/text_service.proto
# Protobuf Python Version: 5.28.1
"""Generated protocol buffer code."""
from google.protobuf import descriptor as _descriptor
from google.protobuf import descriptor_pool as _descriptor_pool
from google.protobuf import runtime_version as _runtime_version
from google.protobuf import symbol_database as _symbol_database
from google.protobuf.internal import builder as _builder
_runtime_version.ValidateProtobufRuntimeVersion(
    _runtime_version.Domain.PUBLIC,
    5,
    28,
    1,
    '',
    'google/ai/generativelanguage/v1beta2/text_service.proto'
)
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()


from google.ai.generativelanguage.v1beta2 import citation_pb2 as google_dot_ai_dot_generativelanguage_dot_v1beta2_dot_citation__pb2
from google.ai.generativelanguage.v1beta2 import safety_pb2 as google_dot_ai_dot_generativelanguage_dot_v1beta2_dot_safety__pb2
from google.api import annotations_pb2 as google_dot_api_dot_annotations__pb2
from google.api import client_pb2 as google_dot_api_dot_client__pb2
from google.api import field_behavior_pb2 as google_dot_api_dot_field__behavior__pb2
from google.api import resource_pb2 as google_dot_api_dot_resource__pb2


DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n7google/ai/generativelanguage/v1beta2/text_service.proto\x12$google.ai.generativelanguage.v1beta2\x1a\x33google/ai/generativelanguage/v1beta2/citation.proto\x1a\x31google/ai/generativelanguage/v1beta2/safety.proto\x1a\x1cgoogle/api/annotations.proto\x1a\x17google/api/client.proto\x1a\x1fgoogle/api/field_behavior.proto\x1a\x19google/api/resource.proto\"\xd0\x03\n\x13GenerateTextRequest\x12>\n\x05model\x18\x01 \x01(\tB/\xe0\x41\x02\xfa\x41)\n\'generativelanguage.googleapis.com/Model\x12\x45\n\x06prompt\x18\x02 \x01(\x0b\x32\x30.google.ai.generativelanguage.v1beta2.TextPromptB\x03\xe0\x41\x02\x12\x18\n\x0btemperature\x18\x03 \x01(\x02H\x00\x88\x01\x01\x12\x1c\n\x0f\x63\x61ndidate_count\x18\x04 \x01(\x05H\x01\x88\x01\x01\x12\x1e\n\x11max_output_tokens\x18\x05 \x01(\x05H\x02\x88\x01\x01\x12\x12\n\x05top_p\x18\x06 \x01(\x02H\x03\x88\x01\x01\x12\x12\n\x05top_k\x18\x07 \x01(\x05H\x04\x88\x01\x01\x12L\n\x0fsafety_settings\x18\x08 \x03(\x0b\x32\x33.google.ai.generativelanguage.v1beta2.SafetySetting\x12\x16\n\x0estop_sequences\x18\t \x03(\tB\x0e\n\x0c_temperatureB\x12\n\x10_candidate_countB\x14\n\x12_max_output_tokensB\x08\n\x06_top_pB\x08\n\x06_top_k\"\xf5\x01\n\x14GenerateTextResponse\x12H\n\ncandidates\x18\x01 \x03(\x0b\x32\x34.google.ai.generativelanguage.v1beta2.TextCompletion\x12\x44\n\x07\x66ilters\x18\x03 \x03(\x0b\x32\x33.google.ai.generativelanguage.v1beta2.ContentFilter\x12M\n\x0fsafety_feedback\x18\x04 \x03(\x0b\x32\x34.google.ai.generativelanguage.v1beta2.SafetyFeedback\"\x1f\n\nTextPrompt\x12\x11\n\x04text\x18\x01 \x01(\tB\x03\xe0\x41\x02\"\xe4\x01\n\x0eTextCompletion\x12\x13\n\x06output\x18\x01 \x01(\tB\x03\xe0\x41\x03\x12J\n\x0esafety_ratings\x18\x02 \x03(\x0b\x32\x32.google.ai.generativelanguage.v1beta2.SafetyRating\x12[\n\x11\x63itation_metadata\x18\x03 \x01(\x0b\x32\x36.google.ai.generativelanguage.v1beta2.CitationMetadataB\x03\xe0\x41\x03H\x00\x88\x01\x01\x42\x14\n\x12_citation_metadata\"e\n\x10\x45mbedTextRequest\x12>\n\x05model\x18\x01 \x01(\tB/\xe0\x41\x02\xfa\x41)\n\'generativelanguage.googleapis.com/Model\x12\x11\n\x04text\x18\x02 \x01(\tB\x03\xe0\x41\x02\"o\n\x11\x45mbedTextResponse\x12L\n\tembedding\x18\x01 \x01(\x0b\x32/.google.ai.generativelanguage.v1beta2.EmbeddingB\x03\xe0\x41\x03H\x00\x88\x01\x01\x42\x0c\n\n_embedding\"\x1a\n\tEmbedding\x12\r\n\x05value\x18\x01 \x03(\x02\x32\xf3\x03\n\x0bTextService\x12\x81\x02\n\x0cGenerateText\x12\x39.google.ai.generativelanguage.v1beta2.GenerateTextRequest\x1a:.google.ai.generativelanguage.v1beta2.GenerateTextResponse\"z\xda\x41\x46model,prompt,temperature,candidate_count,max_output_tokens,top_p,top_k\x82\xd3\xe4\x93\x02+\"&/v1beta2/{model=models/*}:generateText:\x01*\x12\xb9\x01\n\tEmbedText\x12\x36.google.ai.generativelanguage.v1beta2.EmbedTextRequest\x1a\x37.google.ai.generativelanguage.v1beta2.EmbedTextResponse\";\xda\x41\nmodel,text\x82\xd3\xe4\x93\x02(\"#/v1beta2/{model=models/*}:embedText:\x01*\x1a$\xca\x41!generativelanguage.googleapis.comB\x9e\x01\n(com.google.ai.generativelanguage.v1beta2B\x10TextServiceProtoP\x01Z^cloud.google.com/go/ai/generativelanguage/apiv1beta2/generativelanguagepb;generativelanguagepbb\x06proto3')

_globals = globals()
_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)
_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'google.ai.generativelanguage.v1beta2.text_service_pb2', _globals)
if not _descriptor._USE_C_DESCRIPTORS:
  _globals['DESCRIPTOR']._loaded_options = None
  _globals['DESCRIPTOR']._serialized_options = b'\n(com.google.ai.generativelanguage.v1beta2B\020TextServiceProtoP\001Z^cloud.google.com/go/ai/generativelanguage/apiv1beta2/generativelanguagepb;generativelanguagepb'
  _globals['_GENERATETEXTREQUEST'].fields_by_name['model']._loaded_options = None
  _globals['_GENERATETEXTREQUEST'].fields_by_name['model']._serialized_options = b'\340A\002\372A)\n\'generativelanguage.googleapis.com/Model'
  _globals['_GENERATETEXTREQUEST'].fields_by_name['prompt']._loaded_options = None
  _globals['_GENERATETEXTREQUEST'].fields_by_name['prompt']._serialized_options = b'\340A\002'
  _globals['_TEXTPROMPT'].fields_by_name['text']._loaded_options = None
  _globals['_TEXTPROMPT'].fields_by_name['text']._serialized_options = b'\340A\002'
  _globals['_TEXTCOMPLETION'].fields_by_name['output']._loaded_options = None
  _globals['_TEXTCOMPLETION'].fields_by_name['output']._serialized_options = b'\340A\003'
  _globals['_TEXTCOMPLETION'].fields_by_name['citation_metadata']._loaded_options = None
  _globals['_TEXTCOMPLETION'].fields_by_name['citation_metadata']._serialized_options = b'\340A\003'
  _globals['_EMBEDTEXTREQUEST'].fields_by_name['model']._loaded_options = None
  _globals['_EMBEDTEXTREQUEST'].fields_by_name['model']._serialized_options = b'\340A\002\372A)\n\'generativelanguage.googleapis.com/Model'
  _globals['_EMBEDTEXTREQUEST'].fields_by_name['text']._loaded_options = None
  _globals['_EMBEDTEXTREQUEST'].fields_by_name['text']._serialized_options = b'\340A\002'
  _globals['_EMBEDTEXTRESPONSE'].fields_by_name['embedding']._loaded_options = None
  _globals['_EMBEDTEXTRESPONSE'].fields_by_name['embedding']._serialized_options = b'\340A\003'
  _globals['_TEXTSERVICE']._loaded_options = None
  _globals['_TEXTSERVICE']._serialized_options = b'\312A!generativelanguage.googleapis.com'
  _globals['_TEXTSERVICE'].methods_by_name['GenerateText']._loaded_options = None
  _globals['_TEXTSERVICE'].methods_by_name['GenerateText']._serialized_options = b'\332AFmodel,prompt,temperature,candidate_count,max_output_tokens,top_p,top_k\202\323\344\223\002+\"&/v1beta2/{model=models/*}:generateText:\001*'
  _globals['_TEXTSERVICE'].methods_by_name['EmbedText']._loaded_options = None
  _globals['_TEXTSERVICE'].methods_by_name['EmbedText']._serialized_options = b'\332A\nmodel,text\202\323\344\223\002(\"#/v1beta2/{model=models/*}:embedText:\001*'
  _globals['_GENERATETEXTREQUEST']._serialized_start=317
  _globals['_GENERATETEXTREQUEST']._serialized_end=781
  _globals['_GENERATETEXTRESPONSE']._serialized_start=784
  _globals['_GENERATETEXTRESPONSE']._serialized_end=1029
  _globals['_TEXTPROMPT']._serialized_start=1031
  _globals['_TEXTPROMPT']._serialized_end=1062
  _globals['_TEXTCOMPLETION']._serialized_start=1065
  _globals['_TEXTCOMPLETION']._serialized_end=1293
  _globals['_EMBEDTEXTREQUEST']._serialized_start=1295
  _globals['_EMBEDTEXTREQUEST']._serialized_end=1396
  _globals['_EMBEDTEXTRESPONSE']._serialized_start=1398
  _globals['_EMBEDTEXTRESPONSE']._serialized_end=1509
  _globals['_EMBEDDING']._serialized_start=1511
  _globals['_EMBEDDING']._serialized_end=1537
  _globals['_TEXTSERVICE']._serialized_start=1540
  _globals['_TEXTSERVICE']._serialized_end=2039
# @@protoc_insertion_point(module_scope)
